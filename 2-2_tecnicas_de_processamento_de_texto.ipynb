{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a Técnicas de Processamento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Eu gosto de assistir jogos de futebol',\n",
    "        'Já eu, prefiro assistir jogos de basquete'\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu gosto de assistir jogos de futebol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Já eu, prefiro assistir jogos de basquete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text\n",
       "0      Eu gosto de assistir jogos de futebol\n",
       "1  Já eu, prefiro assistir jogos de basquete"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tonekização dos dados\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1\n",
      "assistir  1  1\n",
      "basquete  0  1\n",
      "de        2  1\n",
      "eu        1  1\n",
      "futebol   1  0\n",
      "gosto     1  0\n",
      "jogos     1  1\n",
      "já        0  1\n",
      "prefiro   0  1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0  1\n",
      "assistir jogos    1  1\n",
      "de assistir       1  0\n",
      "de basquete       0  1\n",
      "de futebol        1  0\n",
      "eu gosto          1  0\n",
      "eu prefiro        0  1\n",
      "gosto de          1  0\n",
      "jogos de          1  1\n",
      "já eu             0  1\n",
      "prefiro assistir  0  1\n"
     ]
    }
   ],
   "source": [
    "# podemos criar bigramas e trigramas apenas alterando os valores do parâmetro ngram_range\n",
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0  1\n",
      "assistir jogos de       1  1\n",
      "de assistir jogos       1  0\n",
      "eu gosto de             1  0\n",
      "eu prefiro assistir     0  1\n",
      "gosto de assistir       1  0\n",
      "jogos de basquete       0  1\n",
      "jogos de futebol        1  0\n",
      "já eu prefiro           0  1\n",
      "prefiro assistir jogos  0  1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(3,3))\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_stdlib_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/giuliana/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'futebol',\n",
       " 'brasileiro',\n",
       " 'é',\n",
       " 'o',\n",
       " 'melhor',\n",
       " 'do',\n",
       " 'mundo',\n",
       " '.',\n",
       " 'Você',\n",
       " 'concorda',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "exemplo = 'O futebol brasileiro é o melhor do mundo. Você concorda?'\n",
    "words = word_tokenize(exemplo)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queen, Aerosmith & Beatles -> ['Queen', 'Aerosmith', 'Beatles']\n",
      "Phone Num :  2004-959-559 \n",
      "Phone Num :  2004959559\n"
     ]
    }
   ],
   "source": [
    "#regex\n",
    "import re\n",
    "rex = re.compile('\\w+') \n",
    "bandas = 'Queen, Aerosmith & Beatles'\n",
    "print (bandas, '->', rex.findall(bandas))\n",
    "phone = \"2004-959-559 # This is Phone Number\"\n",
    "num = re.sub('#.*$', \"\", phone) \n",
    "print (\"Phone Num : \", num)\n",
    "num = re.sub(r'\\D', \"\", phone)\n",
    "print (\"Phone Num : \", num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giuliana/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'à',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'às',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'é',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'éramos',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'estávamos',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estivéramos',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'fôramos',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fôssemos',\n",
       " 'fui',\n",
       " 'há',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'hão',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houverá',\n",
       " 'houveram',\n",
       " 'houvéramos',\n",
       " 'houverão',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houveríamos',\n",
       " 'houvermos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéssemos',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'não',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nós',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'são',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'só',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'tém',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tínhamos',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tivéramos',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'carro', 'que', 'estava', 'quebrado', 'voltou', 'a', 'funcionar']\n",
      "['Meu', 'carro', 'quebrou', 'e', 'não', 'está', 'funcionando']\n"
     ]
    }
   ],
   "source": [
    "ex1 = 'O carro que estava quebrado voltou a funcionar'\n",
    "ex2 = 'Meu carro quebrou e não está funcionando'\n",
    "\n",
    "print(word_tokenize(ex1))\n",
    "print(word_tokenize(ex2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'text':[ex1,ex2]})\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1),stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0  1\n",
      "carro        1  1\n",
      "funcionando  0  1\n",
      "funcionar    1  0\n",
      "quebrado     1  0\n",
      "quebrou      0  1\n",
      "voltou       1  0\n"
     ]
    }
   ],
   "source": [
    "stops = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
    "vect.fit(df.text)\n",
    "text_vect = vect.transform(df.text)\n",
    "\n",
    "print(pd.DataFrame(text_vect.A, columns=vect.get_feature_names_out()).T.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giuliana/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stemming & lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "go\n",
      "go\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "examples = ['go', 'going', 'goes', 'gone', 'went']\n",
    "\n",
    "wnl = WordNetLemmatizer() \n",
    "for word in examples:\n",
    "    print(wnl.lemmatize(word, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n",
      "connect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "examples = ['connection', 'connections', 'connective', 'connecting', 'connected']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for word in examples:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conecta\n",
      "conectado\n",
      "conectamo\n",
      "desconectado\n",
      "conectividad\n"
     ]
    }
   ],
   "source": [
    "# porém em português os resultados não são bons \n",
    "\n",
    "examples = ['conecta', 'conectado', 'conectamos', 'desconectados', 'conectividade']\n",
    "\n",
    "for word in examples:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /Users/giuliana/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "# a alternativa é usar o RSLPStemmer, que produz melhores resultados\n",
    "nltk.download('rslp')\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "rslp = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conect\n",
      "conect\n",
      "conect\n",
      "desconect\n",
      "conect\n"
     ]
    }
   ],
   "source": [
    "for word in examples: \n",
    "    print(rslp.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
